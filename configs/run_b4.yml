model:
    cls: DiT_B_4

dataset:
    root: YOUR_DATA_ROOT

training:
    log_per_step: 100
    checkpoint_per_epoch: 10

    num_epochs: 240
    learning_rate: 0.0001
    batch_size: 256
    adam_b2: 0.95
    ema_type: 'const'

    sample_on_training: True
    sample_per_epoch: 10
    fid_per_epoch: 10

    seed: 42

fid:
    device_batch_size: 20
    num_samples: 50000
    cache_ref: YOUR_FID_CACHE_REF

sampling:
    num_steps: 1

# Load checkpoints for eval or resuming training.
# load_from: CKPT_PATH

load_from: null
eval_only: False
